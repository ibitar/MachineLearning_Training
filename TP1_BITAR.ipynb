{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ctJOkCb59Ami"
   },
   "source": [
    "# TP1 Machine Learning - Année 4 - ESILV - Septembre 2023\n",
    "Ibrahim BITAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v_-eu0DKmEt1"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- numpy et pandas sont deux bibliothèques Python populaires pour la manipulation de données. numpy est principalement utilisé pour effectuer des opérations numériques sur des tableaux, tandis que pandas est utilisé pour travailler avec des données tabulaires (comme les données CSV) en utilisant des structures de données appelées DataFrames.\n",
    "\n",
    "\n",
    "- ColumnTransformer est une classe fournie par la bibliothèque scikit-learn (sklearn). Elle permet de gérer différentes transformations sur les colonnes d'un jeu de données, ce qui est utile lorsque vous avez des colonnes avec différents types de données (numériques, catégoriques, textuelles) et que vous souhaitez les prétraiter de manière spécifique.\n",
    "\n",
    "- Pipeline est une classe de scikit-learn qui permet de créer un flux de travail de traitement des données et d'apprentissage automatique en chaînant plusieurs étapes. Vous pouvez spécifier une séquence d'opérations à effectuer, du prétraitement des données à l'apprentissage du modèle, le tout dans une seule structure.\n",
    "\n",
    "- SimpleImputer est utilisé pour gérer les valeurs manquantes dans les données en leur attribuant une valeur spécifique (par exemple, la moyenne des valeurs non manquantes).\n",
    "\n",
    "- StandardScaler est utilisé pour mettre à l'échelle les caractéristiques numériques afin qu'elles aient une moyenne de zéro et un écart-type de un. Cela est souvent nécessaire pour de nombreux algorithmes d'apprentissage automatique.\n",
    "\n",
    "- OneHotEncoder est utilisé pour transformer des variables catégorielles en variables binaires (0 ou 1) pour qu'elles puissent être utilisées dans les modèles d'apprentissage automatique.\n",
    "\n",
    "- LogisticRegression est un modèle d'apprentissage automatique couramment utilisé pour la classification binaire.\n",
    "train_test_split est une fonction de scikit-learn qui est utilisée pour diviser un ensemble de données en ensembles d'entraînement et de test, ce qui est essentiel pour évaluer la performance d'un modèle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "guIMQnm4puGX"
   },
   "source": [
    "As you can remark, a sample-data repository is created after running the previous code. You have access to california_housing_test.csv, california_housing_train.csv, mnist_test.csv and mnist_train_small.csv. You can import a local csv files to your sample_data repo. Let import now the winequality-red.csv that you can download to your local computer from this link  here: https://archive.ics.uci.edu/dataset/186/wine+quality\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hyE2pVd-qdGr",
    "outputId": "e4778460-f60f-4e3a-c581-837c46c46e3d"
   },
   "outputs": [],
   "source": [
    "wine = pd.read_csv(\"data/winequality-red.csv\", sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8R85DWFR26hD",
    "outputId": "6f09d3c5-3a27-441d-9c87-fdb00ce65470"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "group_names = ['bad', 'good']\n",
    "catego = pd.cut(wine['quality'], bins=2, labels=group_names)\n",
    "label_quality = preprocessing.LabelEncoder()\n",
    "wine['quality'] = label_quality.fit_transform(catego)\n",
    "print(wine['quality'].value_counts())\n",
    "print(wine['quality'].head(20))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pd.read_csv est une fonction de la bibliothèque pandas (pd) qui est utilisée pour lire un fichier CSV et le stocker dans un DataFrame, une structure de données tabulaire en deux dimensions.\n",
    "\n",
    "- \"data/winequality-red.csv\" est un chemin relatif vers le fichier CSV. Assurez-vous que le fichier se trouve dans un dossier `data` à la racine du projet.\n",
    "\n",
    "- Pour récupérer ce fichier si besoin, vous pouvez le télécharger depuis le [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv). Par exemple : `wget -O data/winequality-red.csv https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv`.\n",
    "\n",
    "- sep=';' spécifie le séparateur de colonnes dans le fichier CSV. Dans ce cas, le point-virgule (;) est utilisé comme séparateur. Par défaut, la virgule (,) est utilisée comme séparateur, mais cela peut varier selon les fichiers CSV."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "wine.info()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "wine.describe()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "wine.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(wine.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wine.head(3) est utilisé pour afficher les trois premières lignes du DataFrame wine. La méthode head() est couramment utilisée pour afficher un aperçu des premières lignes d'un DataFrame et est utile pour vérifier rapidement les données et s'assurer qu'elles ont été chargées correctement."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rf9Q9svo9Jsg"
   },
   "source": [
    "### Histogrammes des caractéristiques\n",
    "Ces diagrammes permettent de visualiser la distribution de chaque variable pour repérer tendances générales et valeurs aberrantes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 860
    },
    "id": "0lB8ZYeO8WFj",
    "outputId": "b4bfe2d0-e7a1-41a0-bf32-b31e86fde65c"
   },
   "outputs": [],
   "source": [
    "wine.hist(bins=20, figsize=(15,10))\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- wine.hist() génère un histogramme pour chaque variable numérique du DataFrame.\n",
    "- bins=20 spécifie le nombre de barres de chaque histogramme.\n",
    "- figsize=(15, 10) définit la taille de l'ensemble des graphiques.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 430
    },
    "id": "z2G6YycqC9zp",
    "outputId": "e6c96cdc-d49b-44af-d058-38d53cb56e27"
   },
   "outputs": [],
   "source": [
    "wine.plot(kind='box', subplots=True, layout=(3,4), sharex=False, sharey=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 438
    },
    "id": "dt-SSqF5I5EA",
    "outputId": "659ab3d4-7b8a-48ce-8d2d-6058598d2334"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "sns.heatmap(wine.corr(), annot=True, cmap='coolwarm')\n",
    "plt.title('Matrice de corrélation')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cette heatmap met en évidence les corrélations entre les variables : les couleurs vives indiquent des relations fortes, positives ou négatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 461
    },
    "id": "QzzzSrq9MFeO",
    "outputId": "09c442da-b7bc-4b17-a73e-e2989cae2a6f"
   },
   "outputs": [],
   "source": [
    "from pandas.plotting import scatter_matrix\n",
    "scatter_matrix(wine,figsize=(20,20))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Chaque case de la matrice contient un diagramme de dispersion qui montre la relation entre deux variables numériques. La diagonale de la matrice contient des histogrammes de chaque variable par elle-même.\n",
    "\n",
    "- Cette matrice permet de visualiser les relations entre toutes les paires de variables numériques dans le jeu de données sur la qualité du vin. Elle est utile pour détecter des tendances, des regroupements ou des valeurs aberrantes dans les données. Chaque diagramme de dispersion montre comment deux variables numériques sont distribuées et s'il existe une relation linéaire entre elles."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VB-oBkfPfnXy"
   },
   "source": [
    "SKlearn Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-byNzLh3fuIB"
   },
   "outputs": [],
   "source": [
    "#import classes\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remarques \n",
    "### StandardScaler \n",
    "C'est une classe de scikit-learn qui est utilisée pour mettre à l'échelle les caractéristiques numériques d'un ensemble de données. La mise à l'échelle est effectuée en soustrayant la moyenne de chaque caractéristique et en divisant par l'écart-type, ce qui permet de centrer les données autour de zéro et de les mettre à la même échelle.\n",
    "\n",
    "### PCA \n",
    "C'est une classe de scikit-learn qui représente l'analyse en composantes principales (ACP). L'ACP est une technique de réduction de dimensionnalité qui permet de projeter les données dans un nouvel espace de caractéristiques tout en conservant le maximum d'information possible. Elle est couramment utilisée pour réduire la complexité des données.\n",
    "\n",
    "### RandomForestClassifier \n",
    "C'est une classe de scikit-learn qui représente un modèle d'apprentissage automatique basé sur une forêt aléatoire pour la classification. Une forêt aléatoire est un ensemble d'arbres de décision, où chaque arbre est entraîné sur un sous-ensemble aléatoire des données. C'est un algorithme puissant pour la classification et la régression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8vFMZtW69DHJ"
   },
   "source": [
    "Pipeline creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9Ko2ZdMfxOe"
   },
   "outputs": [],
   "source": [
    "#create the pipeline\n",
    "ML_pipeline = make_pipeline(StandardScaler(),\n",
    "                        PCA(n_components=4),\n",
    "                        RandomForestClassifier(criterion='entropy', n_estimators=100, max_depth=5, random_state=1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- StandardScaler() est la première étape du pipeline. Elle normalise (met à l'échelle) les données d'entrée en soustrayant la moyenne et en divisant par l'écart-type. Cela est important pour de nombreux algorithmes d'apprentissage automatique, car il est préférable que toutes les caractéristiques aient la même échelle.\n",
    "\n",
    "- PCA(n_components=4) C'est la deuxième étape du pipeline. Elle effectue l'analyse en composantes principales (ACP) pour réduire la dimension des données à 4 composantes principales. L'ACP est utilisée pour la réduction de dimensionnalité et la création de nouvelles caractéristiques qui capturent le maximum d'information à partir des caractéristiques originales.\n",
    "\n",
    "- RandomForestClassifier(criterion='entropy', n_estimators=100, max_depth=5, random_state=1) est la troisième étape du pipeline. Elle utilise un modèle de forêt aléatoire pour la classification. Les hyperparamètres spécifiés incluent le critère de division des arbres (entropy), le nombre d'estimateurs (arbres) dans la forêt (100), la profondeur maximale des arbres (5), et la graine aléatoire (random_state=1) pour garantir la reproductibilité des résultats."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "X = wine.loc[:, 'fixed acidity':'alcohol']  # or: wine.drop('quality', axis=1)\n",
    "y = wine['quality']\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, stratify=y, random_state=1)\n",
    "ML_pipeline.fit(X_train, y_train)\n",
    "y_pred = ML_pipeline.predict(X_test)\n",
    "test_acc = ML_pipeline.score(X_test, y_test)\n",
    "print(f'Test accuracy: {test_acc:.3f}')\n",
    "diff = (y_test == y_pred)\n",
    "diff\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Évaluation du modèle\n",
    "labels = label_quality.transform(label_quality.classes_)\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred):.3f}\")\n",
    "print(\"Classification report:\n\", classification_report(y_test, y_pred, labels=labels, target_names=label_quality.classes_))\n",
    "cm = confusion_matrix(y_test, y_pred, labels=labels)\n",
    "plt.figure(figsize=(5,4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "            xticklabels=label_quality.classes_,\n",
    "            yticklabels=label_quality.classes_)\n",
    "plt.xlabel('Prédit')\n",
    "plt.ylabel('Réel')\n",
    "plt.title('Matrice de confusion')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Les valeurs de précision, rappel et F1-score fournies par le rapport de classification permettent d'évaluer la qualité du modèle.\n",
    "La matrice de confusion illustre les prédictions correctes (diagonale) et les erreurs entre les classes 'bad' et 'good'.\n",
    "Une concentration des valeurs sur la diagonale principale indique une bonne performance, tandis que des valeurs hors diagonale révèlent les types d'erreurs restantes."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import joblib\n",
    "joblib.dump(ML_pipeline, 'rf_classifier.pkl')\n",
    "# To load: RFmodel = joblib.load('rf_classifier.pkl')\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}